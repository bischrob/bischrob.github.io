[
  {
    "objectID": "posts/why-I-like-R/why-I-like-R.html",
    "href": "posts/why-I-like-R/why-I-like-R.html",
    "title": "Why I like R — web scraping and the NBA playoffs",
    "section": "",
    "text": "Learning R has been fun for a number of reasons for me. One is simply that I find programming fun. It’s like solving puzzles for me. Sure there’s a lot of frustration involved, but I find it worthwhile. R has been fun because I love numbers and it’s designed for statistics. Another passion is efficiency. I don’t like to do things by hand if I have to, and I love to automate things. Today I wanted to demonstrate some of the possibilities R has to offer. Instead of something archaeology related, I’ll talk about another passion of mine–sports.\nThe NBA playoffs are about to start and my beloved Utah Jazz are racing for the number one seed. I frequently check fivethirtyeight’s and ESPN’s playoff predictions. These are great, but they don’t show the odds of ending up in a particular playoff spot. I decided to sit down and see if I could calculate the odds before the game against Golden State ended tonight.\nIf you’re not interested in the code, then skip to the end, and I’ll tell you the numbers. This isn’t an introduction to R, it’s more me showing some of the capabilities of R. The great thing is, you can adapt this code to see the odds your team hits a certain number of wins.\nFirst I love the tidyverse, which is a collection of packages that make R easier to use and understand. I also like some of the features of the magrittr package that aren’t loaded with the tidyverse packages. Rvest is a package to scrape the web, and lubridate is a great package for working with dates.\nlibrary(magrittr)\nlibrary(tidyverse)\n## -- Attaching packages --------------------------------------- tidyverse 1.3.0 --\n## v ggplot2 3.3.3     v purrr   0.3.4\n## v tibble  3.1.0     v dplyr   1.0.5\n## v tidyr   1.1.3     v stringr 1.4.0\n## v readr   1.4.0     v forcats 0.5.1\n## -- Conflicts ------------------------------------------ tidyverse_conflicts() --\n## x tidyr::extract()   masks magrittr::extract()\n## x dplyr::filter()    masks stats::filter()\n## x dplyr::lag()       masks stats::lag()\n## x purrr::set_names() masks magrittr::set_names()\nlibrary(rvest)\n## \n## Attaching package: 'rvest'\n## The following object is masked from 'package:readr':\n## \n##     guess_encoding\nlibrary(lubridate)\n## \n## Attaching package: 'lubridate'\n## The following objects are masked from 'package:base':\n## \n##     date, intersect, setdiff, union\nI decided to use ESPN’s BPI game odds. These are adjusted with lots of variables and are decently accurate.\nFirst, I grabbed the Utah Jazz schedule.\nThere’s only one table on the page so it wasn’t hard to access, but I did have to clean up the data a little bit due to a postponed game and an extra header row. I originally filtered those values here, but I needed the original values for the next section.\nNext, I had to get the links to each remaining game so I could pull the odds of winning.\nThe %&lt;&gt;% function is one of my favorites as it takes whatever is on the left, uses it in the function to the right and also assigns the result of the function back to that variable. Thus I can save the typing for schedule = schedule %&gt;% and it makes it easy to test code before saving it to a variable by just using %&gt;% for the test and then adding in the last &lt; symbol once I have it right.\nI won’t describe what xpaths are in detail, but they can be used to identify specific elements on a page. The table rows match up to the xpath so I can get all of the links to the individual games by just changing what is essentially the row number. This was easier to do before the game started, but afterwards the link disappeared, so I had to find a workaround that works more consistently.\nurl = \"https://www.espn.com/nba/team/schedule/_/name/utah\" %&gt;% \n  read_html()\n\nschedule = url %&gt;%\n  html_node(\"table\") %&gt;%\n  html_table(header = T)\n\ngameIDs = map(1:nrow(schedule),~{\n  xpath = paste0('//*[@id=\"fittPageContainer\"]/div[2]/div[5]/div/div/section/div/section/section/div/div/div/div[2]/table/tbody/tr[',.x+1,']/td[3]/span/a')\nurl %&gt;% \n  html_elements(xpath = xpath) %&gt;% \n  html_attr(\"href\")\n})\nschedule %&lt;&gt;%\n  mutate(gameID = gameIDs) %&gt;% \n  filter(DATE != 'DATE',RESULT != 'Postponed')\nNext, I identified the remaining games. There are a few ways to do this, but I decided to get complicated and convert the date in the schedule table to a real date so I could filter for games today and later. It might be better to just filter for games that are not completed, but this way shows how R can be used for time series.\nschedule %&lt;&gt;% \n  mutate_at(vars(DATE),list(~.x %&gt;% \n                              str_remove_all(\"^.*?,\") %&gt;%\n                              trimws %&gt;% \n                              parse_date_time(\"Om d\"))) %&gt;% \n  mutate(DATE = case_when(month(DATE) == 12~`year&lt;-`(DATE, 2020),\n                          TRUE~`year&lt;-`(DATE, 2021)))\n\nremaining = schedule %&gt;% \n  filter(DATE &gt;= today())\nNext, I used a purrr map function to go through each game link and get the odds of winning for the home team. I then calculated the Jazz odds of winning by determining who was the home team and inverting the odds if necessary. Because I ran this during a game the game link was missing so I used the invalidate function from gtools to add in the odds for tonight. I haven’t used this function much but it seems a good catchall for values that are problematic (e.g., null,NA, or empty values).\nremaining %&lt;&gt;% \n  mutate(HomePred = map_chr(gameID,~{\n    if(gtools::invalid(.x)){\n      result = '49%' # this was the odds pregame for Jazz vs Warriors\n    } else {\n    xpath = \n      '//*[@id=\"gamepackage-predictor\"]/div/div/div[1]/div[1]/div/div/span[1]'\n    result = .x %&gt;% read_html() %&gt;% html_node(xpath = xpath) %&gt;% html_text()\n    }\n    return(result)\n  })) %&gt;% \n  mutate_at(vars(HomePred),list(~.x %&gt;%\n                                  sub(\"%\",\"\",.) %&gt;%\n                                  as.double())) %&gt;% \n  mutate(WinPer = case_when(str_detect(OPPONENT,\"@\")~(100-HomePred)/100,\n                            TRUE~HomePred/100))\nLast, I created a function to simulate the remaining wins using 1 as a win and zero as a loss and replicated that function 10,000 times. The simulation went fairly quickly on my computer, and I used the nice and simple tictoc package to show the time elapsed.\npredictWins = function(probs = remaining$WinPer){\n  map_int(probs,~{\n    sample(1:0,1,replace = T,prob = c(.x,1-.x)) %&gt;% sum\n  }) %&gt;% sum\n}\n\ntictoc::tic()\nsims = replicate(10000,predictWins(remaining$WinPer))\ntictoc::toc()\n## 1.14 sec elapsed\nWith these results I could look at the odds Utah wins its remaining games.\nprop.table(table(sims))\n## sims\n##      0      1      2      3      4 \n## 0.0085 0.0873 0.2913 0.4208 0.1921\nThe Phoenix Suns are closing in on the Jazz. I could run the above code again but substituting the Phoenix Suns schedule url for the Jazz url. As a general rule, copying and pasting code is a bad idea. If I have to fix something in the code, then I have to fix it in multiple places. Instead I can turn everything I did into a function.\npredictRemaining = function(team){\n  \nurl = glue::glue(\"https://www.espn.com/nba/team/schedule/_/name/{team}\") %&gt;% \n  read_html()\n\nschedule = url %&gt;%\n  html_node(\"table\") %&gt;%\n  html_table(header = T)\n\ngameIDs = map(1:nrow(schedule),~{\n  xpath = paste0('//*[@id=\"fittPageContainer\"]/div[2]/div[5]/div/div/section/div/section/section/div/div/div/div[2]/table/tbody/tr[',.x+1,']/td[3]/span/a')\nurl %&gt;% \n  html_elements(xpath = xpath) %&gt;% \n  html_attr(\"href\")\n})\nschedule %&lt;&gt;%\n  mutate(gameID = gameIDs) %&gt;% \n  filter(DATE != 'DATE',RESULT != 'Postponed')\n\nschedule %&lt;&gt;% \n  mutate_at(vars(DATE),list(~.x %&gt;% \n                              str_remove_all(\"^.*?,\") %&gt;%\n                              trimws %&gt;% \n                              parse_date_time(\"Om d\"))) %&gt;% \n  mutate(DATE = case_when(month(DATE) == 12~`year&lt;-`(DATE, 2020),\n                          TRUE~`year&lt;-`(DATE, 2021)))\n\nremaining = schedule %&gt;% \n  filter(DATE &gt;= today())\n\nremaining %&lt;&gt;% \n  mutate(HomePred = map_chr(gameID,~{\n    if(gtools::invalid(.x)){\n      result = '49%' # this was the odds pregame for Jazz vs Warriors\n    } else {\n    xpath = \n      '//*[@id=\"gamepackage-predictor\"]/div/div/div[1]/div[1]/div/div/span[1]'\n    result = .x %&gt;% read_html() %&gt;% html_node(xpath = xpath) %&gt;% html_text()\n    }\n    return(result)\n  })) %&gt;% \n  mutate_at(vars(HomePred),list(~.x %&gt;%\n                                  sub(\"%\",\"\",.) %&gt;%\n                                  as.double())) %&gt;% \n  mutate(WinPer = case_when(str_detect(OPPONENT,\"@\")~(100-HomePred)/100,\n                            TRUE~HomePred/100))\n\npredictWins = function(probs = remaining$WinPer){\n  map_int(probs,~{\n    sample(1:0,1,replace = T,prob = c(.x,1-.x)) %&gt;% sum\n  }) %&gt;% sum\n}\n\nsims = replicate(10000,predictWins(remaining$WinPer))\n  \n  return(list(schedule = schedule, remaining = remaining, sims = sims))\n}\nWith this function I can simplify the code and compare the results.\nutah = predictRemaining(\"utah\")\nsum(startsWith(utah$schedule$RESULT,\"W\"))\n## [1] 50\nprop.table(table(utah$sims))\n## \n##      0      1      2      3      4 \n## 0.0087 0.0856 0.3047 0.4127 0.1883\nphoenix = predictRemaining(\"phoenix\")\nsum(startsWith(phoenix$schedule$RESULT,\"W\"))\n## [1] 48\nprop.table(table(phoenix$sims))\n## \n##      0      1      2      3      4 \n## 0.0094 0.0984 0.3198 0.4024 0.1700\nIf Utah wins zero more games then the Suns have a 0.89 chance of getting the one seed.\nIf Utah wins one more game then the Suns have a 0.57 chance of getting the one seed.\nIf Utah wins two more games then the Suns have a 0.17 chance of getting the one seed.\nThe odds of Utah winning two or more games is 0.91 chance of getting the one seed.\nI can calculate the total odds of the Jazz getting the number one seed like by adding two to the Jazz total as that is the current lead. The Suns have the tiebreaker though.\nadj = utah$sims + 2\nround(1 - sum(phoenix$sims &gt;= adj) / length(phoenix$sims),2)\n## [1] 0.89\nThe result is that the Utah Jazz have a 0.89 chance of getting the number one seed. This is pretty close to what I found elsewhere and good news personally.\nThe great thing is I can rerun this code whenever I want and it should work. Of course, that doesn’t always work as intended but at least I learn some new skills every time I encounter a problem. I’ve used R for a few years now, but I still visited eleven different stack overflow questions just to write this."
  },
  {
    "objectID": "posts/Size 3D Models in Blender/Size-3D-Models-in-Blender.html",
    "href": "posts/Size 3D Models in Blender/Size-3D-Models-in-Blender.html",
    "title": "Size 3D Models in Blender",
    "section": "",
    "text": "This tutorial demonstrates how to accurately size a 3D model using the open source software Blender.\nYou will need a 3D model in a common file format (I use OBJs) and the measurement of the longest axis of the object. Once you become familiar with this process it will only take a couple minutes to complete.\n3D models are great for measurements that are difficult to obtain from physical objects, but first the 3D model itself must be the right size. Many laser scanners or structured-light scanners automatically scale the object, but often models created using photogrammetry lack a true scale. Whatever the reason you need a 3D object scaled to the right size, this tutorial will show you a simple method using Blender.\nI’ve chosen Blender as it is free and comes with no restrictions. The software can be used to create an animated movie from scratch or design a 3D game, but it can be overwhelming at first. Blender has a large community and many free tutorials if you would like to become more familiar with the basics or expand your knowledge. This tutorial is designed for the Blender beginner, but I generally will only cover Blender basics related to this process."
  },
  {
    "objectID": "posts/Size 3D Models in Blender/Size-3D-Models-in-Blender.html#step-1---remove-cube",
    "href": "posts/Size 3D Models in Blender/Size-3D-Models-in-Blender.html#step-1---remove-cube",
    "title": "Size 3D Models in Blender",
    "section": "Step 1 - Remove cube",
    "text": "Step 1 - Remove cube\n\n\n\nReside 3D Models in Blender\n\n\nIf you have the default setup for Blender, then the first step is to remove the default cube. This is done by first selecting the object. Right click on the cube and it will be outlined in orange. Then, with the mouse inside the 3D View window (keyboard shortcuts only work if the mouse is hovering in the right area), press X or the delete key. A menu will appear, click delete."
  },
  {
    "objectID": "posts/Size 3D Models in Blender/Size-3D-Models-in-Blender.html#step-2---import-3d-model",
    "href": "posts/Size 3D Models in Blender/Size-3D-Models-in-Blender.html#step-2---import-3d-model",
    "title": "Size 3D Models in Blender",
    "section": "Step 2 - Import 3D model",
    "text": "Step 2 - Import 3D model\n\n\n\nResize 3D Models in Blender\n\n\nThe next step is to import the 3D model (this tutorial uses OBJ, but is similar for other file types). Select the following from the menu: File→Import→Wavefront (.obj).\nWarning! it make take some time to load if the model is large (you may consider decimating the model in the program used to create it or by using Blender itself)."
  },
  {
    "objectID": "posts/Size 3D Models in Blender/Size-3D-Models-in-Blender.html#step-3---adjust-orientation",
    "href": "posts/Size 3D Models in Blender/Size-3D-Models-in-Blender.html#step-3---adjust-orientation",
    "title": "Size 3D Models in Blender",
    "section": "Step 3 - Adjust orientation",
    "text": "Step 3 - Adjust orientation\n\n\n\nImported object\n\n\nOnce the object is loaded it should appear in the view screen. Occasionally the object will not be visible, either due to its small size or that the 3D model is located far from the origin. The easiest way to find the model is to press . on the numpad (note that only the numpad can be used for navigation unless changed in the Blender settings). Another method is to use the toolbar on the bottom of the 3D View window (the lowest screen is defaulted to the timeline window and selections from this toolbar will not affect the 3D view; see a general Blender tutorial for more details). Select View→Align View→View selected to view the object. This only works on the selected object. Another way to select an object is to use the Outliner window located in the top right of the default window. Any object in this window can be selected by left-clicking it.\n\n\n\nOrigin to Geometry\n\n\nIf the imported object looks correct, then next we adjust the origin to make the object easier to manage (assuming the 3D model isn’t tied to a meaningful origin point [it usually isn’t]). The orientation of the selected object can be changed using the Tool Shelf. This panel can be viewed/hidden by pressing T on the keyboard. The first tab on the Tool Shelf is labeled Tools. Select set origin and a menu will appear. Select the first option (Geometry to Origin). This will move the object to the center (use . on the numpad to reorient the 3D Viewer).\nNext we will rotate the object. This step is critical as the orientation of the object will be used for sizing the object. R on the keyboard is used to rotate the object. I find it easiest to rotate one axis at a time. Limit the rotation by pressing R and then X, Y, or Z. There is no need to hold down keys when executing sequences on the keyboard. Simply press R once, then press X once. Changing the view will help with orienting the object. First change the view to ortho by pressing 5 on the numpad (while I will be using keyboard shortcuts the corresponding navigation options are under the View menu at the bottom of the 3D View screen). Next, move to Front view by pressing 1 on the numpad. Rotate the object using the keyboard shortcuts above until the objects looks as it should when viewed from the front. Move the mouse in a circle to rotate it and use the shift key to make fine adjustments. Repeat this step for the side view (3 on the numpad) and top view (7 on the numpad). You can view the opposite side by holding down Ctrl. For example, press Ctrl-T to see the bottom view. You may need to move between the views several times to properly orient the object. Remember to limit the rotation to one axis to avoid changing previously oriented axes. We will scale the object along its longest axis. To do this the longest axis must be aligned directly to the X, Y, or Z axis. I use the Y axis in this tutorial.\n\n\n\nLongest Axis\n\n\nIn the above screenshot the edge of the object farthest above and below the X axis are directly aligned. The green line is the Y axis running through these points. The screenshot shows the top view (7 on the numpad). The Y axis is aligned with the location of the measurement taken from the physical object. The axis lines are a good way to align the model. Drag the arrows in the direction you want to move the object so the longest axis is aligned with the chosen axis (you can also move the object free-form by pressing G and then left-clicking to place the object).\nThe last step in orienting the object is to apply the changes. Failing to apply the changes will mess up the next step. Press Ctrl-A and a menu will appear. Select Rotation & Scale. I also like to apply the Location as well, although this is not necessary."
  },
  {
    "objectID": "posts/Size 3D Models in Blender/Size-3D-Models-in-Blender.html#step-4---scaling",
    "href": "posts/Size 3D Models in Blender/Size-3D-Models-in-Blender.html#step-4---scaling",
    "title": "Size 3D Models in Blender",
    "section": "Step 4 - Scaling",
    "text": "Step 4 - Scaling\n\n\n\nChange Scene Units\n\n\nBlender does not use real world measurements by default. To change this, first select Scene on the properties window located by default on the far right (the icon has a sun, globe, and cylinder). In the Units section select Unit Presets (as shown in the screenshot above) and select Millimeters. Any unit can be used if desired, but millimeter is the default unit for STLs, which is the most commonly used file type for 3D printing. After changing the scene unit the dimensions will now have mm.\nIn the screenshot above the dimensions are shown in the properties panel. This is different from the properties window on the right (I know it’s confusing). If you do not see this tab press N. This panel has a lot of useful information about the selected object, and most of these attributes can be directly modified on this panel. The Rotation values should all be 0° and the scale values should all be 1. If this is not the case, then please see the last part of Step 3.\nTo scale the object simply enter the measurement value in millimeters on the appropriate axis. In my example I have aligned the model so that my physical measurement is directly along the Y axis. The method I demonstrate only works if this is the maximum Y length. In my example, the Y axis is currently showing as 5.551 mm. If part of the model extends further along the Y axis then what I measured the scaling will be off.\n\n\n\nScaling\n\n\nIn the above screenshot, I entered 24mm into the Dimensions box, which is the physical measurement I made. Note that this distorts the model. The Scale now reads 4.323 in the Y axis box. Copy and paste the Y scale to the X and Z boxes using the keyboard shortcuts Ctrl-C and Ctrl-V. You will see that the model is no longer distorted.\nThe final step is to apply these changes. Press Ctrl_A and apply the Scale. The values in the Scale box will now read 1. Now any measurements made on this model will be correct.\nNext you may want to measure something. Here’s a link to get you started — Ruler & Protractor.\nThanks for reading the tutorial and feel free to comment below or email me if you have any questions, or if you have any suggestions on improving this process."
  },
  {
    "objectID": "posts/Getting-my-Powerpoint-Online/Getting-my-Powerpoint-Online.html",
    "href": "posts/Getting-my-Powerpoint-Online/Getting-my-Powerpoint-Online.html",
    "title": "Getting My Powerpoint Online",
    "section": "",
    "text": "Powerpoints are great. I know we like to pretend we hate them, but in a conference setting where the alternative is to just stare at the presenter awkwardly, I prefer to alternate staring awkwardly and, hopefully, look at some pretty pictures and graphs. This year, I thought I’d put my presentation for the Society for American Archaeology (SAA) Annual Meeting presentation online. It may not be the best presentation, but I want to try to be as open with my work as I can.\n\n\n\nTonto Basin Powerpoint\n\n\nI found two ways to do this. First, when I stopped by the wonderful tDAR (tDAR.org) booth, I found out just how easy it is to upload my presentation to their database. I’ve been meaning to do this for a while, but have never gotten around to it. tDAR is a permanent digital repository. Essentially you can store anything digital in their repository and it will be both safe and accessible, if you want it to be. Second, I wanted to put up a copy on this blog. The hard, but potentially fun way would be to export and format the presentation into one of the great HTML-based slide presentations, but I decided to take the easy route and use the web version of powerpoint to embed my presentation, which you can enjoy (or not) below.\nThe tDAR upload is not ready yet, but you can view the GitHub repository here.\nWhile I wait for tDAR to process this year’s abstracts, I have managed to upload all of my previous SAA presentations and organize them into a collection. Now if I want to find an old SAA presentation for whatever reason, I don’t have to dig through old files. I can go to the tDAR link where the files are stored permanently.\n\nThis is an embedded &lt;a target=\"_blank\" href=\"https://office.com\"&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=\"_blank\" href=\"https://office.com/webapps\"&gt;Office&lt;/a&gt;."
  },
  {
    "objectID": "posts/Blog Update/Blog-Update.html",
    "href": "posts/Blog Update/Blog-Update.html",
    "title": "Blog Update",
    "section": "",
    "text": "Today I converted my website to the Quarto format instead of the Jekyll Github Pages format. You can learn more about Quarto blogs here. I did this mostly because it will be easier to manage and update.\nLet me know if you find any errors in a post after the conversion. I can be reached at bischrob@gmail.com."
  },
  {
    "objectID": "posts/Automated-Netlogo_Flowcharts_in_R/Automated-Netlogo_Flowcharts_in_R.html",
    "href": "posts/Automated-Netlogo_Flowcharts_in_R/Automated-Netlogo_Flowcharts_in_R.html",
    "title": "Automated NetLogo Flowcharts in R",
    "section": "",
    "text": "Flow chart"
  },
  {
    "objectID": "posts/Automated-Netlogo_Flowcharts_in_R/Automated-Netlogo_Flowcharts_in_R.html#netlogo",
    "href": "posts/Automated-Netlogo_Flowcharts_in_R/Automated-Netlogo_Flowcharts_in_R.html#netlogo",
    "title": "Automated NetLogo Flowcharts in R",
    "section": "NetLogo",
    "text": "NetLogo\nNetLogo is a popular tool for building agent based models. A helpful group has built a nice R package for interfacing with NetLogo called NLRX. One of the functions that is really fun to play with is the nldoc_network function. It builds an igraph object using the NetLogo code that graphs the procedure calls in the model.\nWe’ll use the Bacterial Infection model as an example.\n\n\n\nBacterial Infection\n\n\nWe need to first find the directory it is in. This will vary by operating system and version. Then we can call the function and get an igraph network.\nlibrary(nlrx, quietly = T, warn.conflicts = F)\ndir = \"C:\\\\Program Files\\\\NetLogo 6.2.2\\\\app\\\\models\\\\Sample Models\\\\Biology\\\\Evolution\"\nmodel = file.path(dir,\"Bacterial Infection.nlogo\")\nnw = nldoc_network(model)\nnw\nWe can plot it with igraph.\nplot(nw)\n\n\n\nigraph plot"
  },
  {
    "objectID": "posts/Academic-Reading-on-the-Go/Academic-Reading-on-the-Go.html",
    "href": "posts/Academic-Reading-on-the-Go/Academic-Reading-on-the-Go.html",
    "title": "Academic Reading on the Go — Adobe’s liquid mode is a game changer",
    "section": "",
    "text": "Academic reading on the go\n\n\n\nI love reading on my phone. Why? Because it is convenient. Books are great, but they usually require using both my hands, and that’s a lot of work. Even worse, I have to carry the book around. It’s been a long time since I walked around with a novel in my pocket in case I got bored. Reading on a computer is ok, but it’s not my favorite. I usually read e-books or html content like news and blogs on my phone. Keeping up with the latest archaeology articles and books is a lot harder though. I often feel like I should be spending a lot more time keeping up with the ever-growing literature. Academic books aren’t very portable, but most of my reading is done with pdfs. For a format that is literally called a “portable document format,” pdfs aren’t that easy to read on a phone. Apparently I’m not alone in thinking this:\n\nReading documents — particularly PDFs — on mobile has never been a stellar experience. According to Adobe’s own research, 65% of people in the U.S. find it frustrating and 45% stopped reading or didn’t try [1].\n\nI’m writing this because I’ve found that if it is too much of a pain to read an article on my phone, then I usually won’t do it unless I have to. This brings me to Adobe’s liquid mode on their mobile Acrobat Reader app. I love Adobe’s products and am lucky to have a subscription but this feature is fortunately free for those who do not have an Adobe subscription.\n\n\n\nstandard PDF view\n\n\nI usually will try to read a pdf in landscape view to try and maximize the screen, but moving your eyes this much slows down reading a lot. Liquid mode is an option at the top of the app that uses Adobe’s Sensei AI to try to convert the pdf to what is essentially an e-book view.\nNow I can adjust the text size however I want and view an autogenerated table of contents for navigation. Even the figures and references look nicely formatted.\n\n\n\nLiquid mode view\n\n\nAs with everything, there are some limitations. Large files aren’t supported (200 pages is what the app currently says). Scanned pdfs usually don’t work either. I’ve been pleasantly surprised with how well it works, but there are still plenty of formatting issues where text appears out of place or bullet points are improperly located.\nIt’s easier though if we don’t have to deal with pdfs in the first place. Many publishers offer html versions of their journal articles. I love this for open source articles but I am hesitant for publishers that require a subscription. First, I have to log in through my institution to even get to the article, which can sometimes be quite the pain. Second, I’m worried I’ll lose access to the article so I just download the pdf anyway.\n\n\n\nLiquid mode examples\n\n\nI’m a big fan of open source, and I love the idea of preprints. My first preprint is now online, but one problem with the preprint server is that I have to upload a pdf. Liquid mode is one solution to this problem, but I wanted to try to improve the reading experience for my preprint. In the abstract of the preprint I placed a link to a bookdown version of the combined article and supplemental files. Bookdown is a way to write books or articles that can be converted to html, pdf, or epub formats. This was my first attempt at using bookdown and I ran into a few problems, but overall I think it is a great way to improve mobile access.\n\n\n\nBookdown version\n\n\nBookdown allows you to easily navigate the document, download a pdf or epub version, change the text size, and even change to sepia or night mode.\nI am a big fan of liquid mode and I’ve already used it to increase my reading productivity, but I also want to encourage authors to consider ease of access in their publishing. Reducing mobile phone usage is a worthy goal, but so is reading the latest preprints while waiting in line at the grocery store.\nLet me know in the comments or via email if you have any tips for reading productively.\n\n\n1 Wiggers, Kyle 2020 Adobe’s Liquid Mode Leverages AI to Reformat PDFs for Mobile Devices. VentureBeat. https://venturebeat.com/2020/09/23/adobes-liquid-mode-leverages-ai-to-reformat-pdfs-for-mobile-devices/, accessed May 8, 2021."
  },
  {
    "objectID": "data/Slides/Final-Project-Proposal-Presentation-html_files/reveal.js-3.3.0.1/plugin/markdown/example.html",
    "href": "data/Slides/Final-Project-Proposal-Presentation-html_files/reveal.js-3.3.0.1/plugin/markdown/example.html",
    "title": "Markdown Demo",
    "section": "",
    "text": "Content 1.1\nNote: This will only appear in the speaker notes window.\n\n\n\nContent 1.2\n\n\n\nContent 2.1\n\n\n\nContent 3.1\n\n\n\nContent 3.2"
  },
  {
    "objectID": "data/Slides/Final-Project-Proposal-Presentation-html_files/reveal.js-3.3.0.1/plugin/markdown/example.html#external-1.1",
    "href": "data/Slides/Final-Project-Proposal-Presentation-html_files/reveal.js-3.3.0.1/plugin/markdown/example.html#external-1.1",
    "title": "Markdown Demo",
    "section": "",
    "text": "Content 1.1\nNote: This will only appear in the speaker notes window."
  },
  {
    "objectID": "data/Slides/Final-Project-Proposal-Presentation-html_files/reveal.js-3.3.0.1/plugin/markdown/example.html#external-1.2",
    "href": "data/Slides/Final-Project-Proposal-Presentation-html_files/reveal.js-3.3.0.1/plugin/markdown/example.html#external-1.2",
    "title": "Markdown Demo",
    "section": "",
    "text": "Content 1.2"
  },
  {
    "objectID": "data/Slides/Final-Project-Proposal-Presentation-html_files/reveal.js-3.3.0.1/plugin/markdown/example.html#external-2",
    "href": "data/Slides/Final-Project-Proposal-Presentation-html_files/reveal.js-3.3.0.1/plugin/markdown/example.html#external-2",
    "title": "Markdown Demo",
    "section": "",
    "text": "Content 2.1"
  },
  {
    "objectID": "data/Slides/Final-Project-Proposal-Presentation-html_files/reveal.js-3.3.0.1/plugin/markdown/example.html#external-3.1",
    "href": "data/Slides/Final-Project-Proposal-Presentation-html_files/reveal.js-3.3.0.1/plugin/markdown/example.html#external-3.1",
    "title": "Markdown Demo",
    "section": "",
    "text": "Content 3.1"
  },
  {
    "objectID": "data/Slides/Final-Project-Proposal-Presentation-html_files/reveal.js-3.3.0.1/plugin/markdown/example.html#external-3.2",
    "href": "data/Slides/Final-Project-Proposal-Presentation-html_files/reveal.js-3.3.0.1/plugin/markdown/example.html#external-3.2",
    "title": "Markdown Demo",
    "section": "",
    "text": "Content 3.2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a computational archaeologist focused on the Southwest and Great Basin in the United States. My research focuses on various computational methods including network analysis, geometric morphometrics, 3D modeling, and GIS. I received my master’s degree from Brigham Young University and am currently attending Arizona State University for my doctorate. I am also working as the database manager for the Center for Archaeology and Society at Arizona State University."
  },
  {
    "objectID": "about.html#me",
    "href": "about.html#me",
    "title": "About",
    "section": "",
    "text": "I am a computational archaeologist focused on the Southwest and Great Basin in the United States. My research focuses on various computational methods including network analysis, geometric morphometrics, 3D modeling, and GIS. I received my master’s degree from Brigham Young University and am currently attending Arizona State University for my doctorate. I am also working as the database manager for the Center for Archaeology and Society at Arizona State University."
  },
  {
    "objectID": "about.html#cv",
    "href": "about.html#cv",
    "title": "About",
    "section": "CV",
    "text": "CV\nDownload CV"
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About",
    "section": "Contact",
    "text": "Contact\nEmail me at bischrob@gmail.com"
  },
  {
    "objectID": "data/Slides/Final-Project-Proposal-Presentation-html_files/reveal.js-3.3.0.1/CONTRIBUTING.html",
    "href": "data/Slides/Final-Project-Proposal-Presentation-html_files/reveal.js-3.3.0.1/CONTRIBUTING.html",
    "title": "Robert J. Bischoff",
    "section": "",
    "text": "Please keep the issue tracker limited to bug reports, feature requests and pull requests.\n\n\nIf you have personal support or setup questions the best place to ask those are StackOverflow.\n\n\n\nWhen reporting a bug make sure to include information about which browser and operating system you are on as well as the necessary steps to reproduce the issue. If possible please include a link to a sample presentation where the bug can be tested.\n\n\n\n\nShould follow the coding style of the file you work in, most importantly:\n\nTabs to indent\nSingle-quoted strings\n\nShould be made towards the dev branch\nShould be submitted from a feature/topic branch (not your master)\n\n\n\n\nPlease do not submit plugins as pull requests. They should be maintained in their own separate repository. More information here: https://github.com/hakimel/reveal.js/wiki/Plugin-Guidelines"
  },
  {
    "objectID": "data/Slides/Final-Project-Proposal-Presentation-html_files/reveal.js-3.3.0.1/CONTRIBUTING.html#contributing",
    "href": "data/Slides/Final-Project-Proposal-Presentation-html_files/reveal.js-3.3.0.1/CONTRIBUTING.html#contributing",
    "title": "Robert J. Bischoff",
    "section": "",
    "text": "Please keep the issue tracker limited to bug reports, feature requests and pull requests.\n\n\nIf you have personal support or setup questions the best place to ask those are StackOverflow.\n\n\n\nWhen reporting a bug make sure to include information about which browser and operating system you are on as well as the necessary steps to reproduce the issue. If possible please include a link to a sample presentation where the bug can be tested.\n\n\n\n\nShould follow the coding style of the file you work in, most importantly:\n\nTabs to indent\nSingle-quoted strings\n\nShould be made towards the dev branch\nShould be submitted from a feature/topic branch (not your master)\n\n\n\n\nPlease do not submit plugins as pull requests. They should be maintained in their own separate repository. More information here: https://github.com/hakimel/reveal.js/wiki/Plugin-Guidelines"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Blog Update\n\n\n\n\n\n\nnews\n\n\n\nConverted website to Quarto.\n\n\n\n\n\nJul 23, 2024\n\n\nRobert J.Bischoff\n\n\n\n\n\n\n\n\n\n\n\n\nAutomated NetLogo Flowcharts in R\n\n\n\n\n\n\nR\n\n\ntutorial\n\n\n\nHow to create flowcharts from NetLogo models in R.\n\n\n\n\n\nDec 8, 2022\n\n\nRobert J.Bischoff\n\n\n\n\n\n\n\n\n\n\n\n\nUseR Conference Shiny Presentation\n\n\n\n\n\n\npresentation\n\n\nR\n\n\n\nLink to UseR CatMapper presentation.\n\n\n\n\n\nJun 21, 2022\n\n\nRobert J.Bischoff\n\n\n\n\n\n\n\n\n\n\n\n\nGetting My Powerpoint Online\n\n\n\n\n\n\ntutorial\n\n\npresentation\n\n\n\n3D models of the Pilling Figurines.\n\n\n\n\n\nApr 3, 2022\n\n\nRobert J.Bischoff\n\n\n\n\n\n\n\n\n\n\n\n\nEasy GitHub Pages Website\n\n\n\n\n\n\ntutorial\n\n\n\n3D models of the Pilling Figurines.\n\n\n\n\n\nMar 11, 2022\n\n\nRobert J.Bischoff\n\n\n\n\n\n\n\n\n\n\n\n\nArtifact Photogrammetry Basics\n\n\n\n\n\n\n3D-Models\n\n\ntutorial\n\n\n\nPhotogrammetry tutorial.\n\n\n\n\n\nJul 15, 2021\n\n\nRobert J.Bischoff\n\n\n\n\n\n\n\n\n\n\n\n\nAcademic Reading on the Go — Adobe’s liquid mode is a game changer\n\n\n\n\n\n\nproductivity\n\n\n\nHow to read pdfs on mobile.\n\n\n\n\n\nMay 8, 2021\n\n\nRobert J.Bischoff\n\n\n\n\n\n\n\n\n\n\n\n\nWhy I like R — web scraping and the NBA playoffs\n\n\n\n\n\n\ntutorial\n\n\nsports\n\n\nR\n\n\n\nR tutorial: web scraping.\n\n\n\n\n\nMay 8, 2021\n\n\nRobert J.Bischoff\n\n\n\n\n\n\n\n\n\n\n\n\nMaking Image Mosaics in R\n\n\n\n\n\n\nR\n\n\ntutorial\n\n\n\nHow to create image mosaics using R.\n\n\n\n\n\nJun 30, 2020\n\n\nRobert J.Bischoff\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatic Background Removal\n\n\n\n\n\n\n3D-Models\n\n\ntutorial\n\n\n\nRemoving image backgrounds automatically using Photoshop.\n\n\n\n\n\nMay 16, 2019\n\n\nRobert J.Bischoff\n\n\n\n\n\n\n\n\n\n\n\n\nSize 3D Models in Blender\n\n\n\n\n\n\n3D-Models\n\n\ntutorial\n\n\n\nHow to accurately size 3D models using Blender.\n\n\n\n\n\nJun 11, 2018\n\n\nRobert J.Bischoff\n\n\n\n\n\n\n\n\n\n\n\n\nPilling Figurines 3D Models\n\n\n\n\n\n\n3D-Models\n\n\n\n3D models of the Pilling Figurines.\n\n\n\n\n\nJun 9, 2018\n\n\nRobert J.Bischoff\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Artifact-Photogrammetry-Basics/Artifact-Photogrammetry-Basics.html",
    "href": "posts/Artifact-Photogrammetry-Basics/Artifact-Photogrammetry-Basics.html",
    "title": "Artifact Photogrammetry Basics",
    "section": "",
    "text": "Photogrammetry is a broad field that involves obtaining metrics from photographs. Structure from motion (often abbreviated sfm) is a branch of photogrammetry focused on estimating 3D structure from photos. This is usually what we mean when we say photogrammetry, and I’m happy to continue using the generic term, but now you know.\n\n\n\nCollage\n\n\nPhotogrammetry is useful in a broad range of fields and is becoming a standard–even common–method in archaeology. 3D models can be used for preservation (artifacts cannot be replaced by 3D models, but much would have been preserved if we had 3D models of all the objects loss in the fire at the Museu Nacional in Rio de Janeiro), for public outreach (the Virtual Curation Laboratory is a great example), and for analysis (primarily through geometric morphometrics).\n\n\n\n3D printing\n\n\n\ncredit Brian McNeill\n\n\nPhotogrammetry is one of many ways to create 3D models, but it is one of the least expensive. Options range from free to use cell phone apps to professional software costing thousands a year for a license (here is a recent look at different software. Agisoft’s Metashape has been around a while (it used to be called Photoscan) and it is what I use the most (I use the standard version purchased with a student license).\nMy purpose here is to give a brief demonstration of my workflow for creating 3D models from archaeological artifacts (applies to any object really). The Metashape manual is quite useful, and there are a lot of good references online. I learned photogrammetry myself through Youtube and Google.\nThere is a major difference between creating a full, 360° model and making a landscape model or modeling a stationary object. Photogrammetry uses reference points in the images to calculate the 3D structure, but if you move something in the background or the object itself, then it creates problems. The problem with modeling an artifact is that you usually need to flip the artifact over to get all sides of it. Once you flip the object over the software will usually create something like this.\n\n\n\nDouble Image\n\n\nThe solution is to tell the software to ignore everything in the background. There are many ways to do this, but the most sure is to make a black and white masks for each image. This is the process I’ll demonstrate, but before you make a 3D model you need to take the pictures.\nAnd before that, you need something to model…"
  },
  {
    "objectID": "posts/Artifact-Photogrammetry-Basics/Artifact-Photogrammetry-Basics.html#photogrammetry",
    "href": "posts/Artifact-Photogrammetry-Basics/Artifact-Photogrammetry-Basics.html#photogrammetry",
    "title": "Artifact Photogrammetry Basics",
    "section": "",
    "text": "Photogrammetry is a broad field that involves obtaining metrics from photographs. Structure from motion (often abbreviated sfm) is a branch of photogrammetry focused on estimating 3D structure from photos. This is usually what we mean when we say photogrammetry, and I’m happy to continue using the generic term, but now you know.\n\n\n\nCollage\n\n\nPhotogrammetry is useful in a broad range of fields and is becoming a standard–even common–method in archaeology. 3D models can be used for preservation (artifacts cannot be replaced by 3D models, but much would have been preserved if we had 3D models of all the objects loss in the fire at the Museu Nacional in Rio de Janeiro), for public outreach (the Virtual Curation Laboratory is a great example), and for analysis (primarily through geometric morphometrics).\n\n\n\n3D printing\n\n\n\ncredit Brian McNeill\n\n\nPhotogrammetry is one of many ways to create 3D models, but it is one of the least expensive. Options range from free to use cell phone apps to professional software costing thousands a year for a license (here is a recent look at different software. Agisoft’s Metashape has been around a while (it used to be called Photoscan) and it is what I use the most (I use the standard version purchased with a student license).\nMy purpose here is to give a brief demonstration of my workflow for creating 3D models from archaeological artifacts (applies to any object really). The Metashape manual is quite useful, and there are a lot of good references online. I learned photogrammetry myself through Youtube and Google.\nThere is a major difference between creating a full, 360° model and making a landscape model or modeling a stationary object. Photogrammetry uses reference points in the images to calculate the 3D structure, but if you move something in the background or the object itself, then it creates problems. The problem with modeling an artifact is that you usually need to flip the artifact over to get all sides of it. Once you flip the object over the software will usually create something like this.\n\n\n\nDouble Image\n\n\nThe solution is to tell the software to ignore everything in the background. There are many ways to do this, but the most sure is to make a black and white masks for each image. This is the process I’ll demonstrate, but before you make a 3D model you need to take the pictures.\nAnd before that, you need something to model…"
  },
  {
    "objectID": "posts/Artifact-Photogrammetry-Basics/Artifact-Photogrammetry-Basics.html#selecting-an-object",
    "href": "posts/Artifact-Photogrammetry-Basics/Artifact-Photogrammetry-Basics.html#selecting-an-object",
    "title": "Artifact Photogrammetry Basics",
    "section": "Selecting an object",
    "text": "Selecting an object\nFactors to consider when selecting an object are object geometry, surface reflection/transparency, sharp/fine edges, and moving/flowing parts. You need to be able to capture all aspects of the object from multiple camera angles. This can be challenging with small orifices or complicated geometry. For example, the Clay Pipe model below was challenging because I could not capture very far into the interior of the pipe. The size of the object makes a big difference but only because of the difficulty photographing it. If you can take a picture of it then then you can make a 3D model, but you may need special equipment for very small objects. A macro lens is a good investment.\n\n\n\n\n\nTransparent and reflective images will not work for photogrammetry due to the light distortion, but you can coat the surface with something non-reflective. I’ve tried talcum powder and it worked, but it wasn’t my favorite (see Clovis point below for my talcum example–note that the obsidian looking surface in the 3D model was made using Blender. This is a good study comparing different options.\n\n\n\nObsidian Coating\n\n\n\nClovis point coated in talcum powder\n\n\n\n\n\n\n\nAnother challenge is fine particles like hair or fur or sharp edges. The sharp edges of the obsidian Clovis point took several attempts to capture, but I found that taking lots of pictures helped. Anything that moves freely like hair or fur will be a challenge if it is not in the same relative position from photo to photo. This cradle figurine is a good example of how fur is difficult to capture. You can see that the edges are much fuzzier than other parts of the model. There are also holes where it was difficult to photograph the object due to the angles. I also didn’t want to set the basket upside down to photograph the back so that part is missing. To make the model seem nicer I added a board using Blender to make it look like it was sitting on something.\n\n\n\n\n\nTo start with, I recommend choosing an object with rough texture and relatively simple geometry. It should have some distinguishing features though. I’ve found clay figures work very well. Here is a dataset that I will use in this tutorial. These are pictures of Pillings Figurine No. 3 which is on display at the USU Eastern Prehistoric Museum. While choosing a good artifact is helpful for learning, don’t give up! I have been able to make 3D models work in some pretty bad situations–I even wrote a post about it."
  },
  {
    "objectID": "posts/Artifact-Photogrammetry-Basics/Artifact-Photogrammetry-Basics.html#photography",
    "href": "posts/Artifact-Photogrammetry-Basics/Artifact-Photogrammetry-Basics.html#photography",
    "title": "Artifact Photogrammetry Basics",
    "section": "Photography",
    "text": "Photography\nThis is the most important step! It doesn’t matter how good your software is if your photos are garbage or you don’t have enough of them. I’ll be brief in a section that could use a lengthy discussion, and I still have a lot to learn myself.\n\nEquipment\nI’ll list the optimal equipment here, but if you don’t have it then do without:\n\nDSLR Camera\nMacro lens\nTripod\nRemote shutter release\nLighting\nTurntable\n\n\n\nSetup\nBecause you are removing the background, you want something as uniform as possible (think green screen technology but if you use green you may have to correct for the green glare). Choose something that contrasts with the color of the object so you can easily remove the background later.\nYou want the camera stationary for best results, so use a tripod. The remote shutter release is great so that you don’t bump the camera while taking pictures.\nLighting is always the most important element in photography. You want diffuse light that doesn’t cause harsh shadows. In fact, try to avoid all shadows (yes this can be a challenge–light boxes are one option).\nThe turntable makes life a lot easier. You can even put marks on it to tell you how far to rotate the table in between shots. I’ve never used one, but an automatic turntable setup would be amazing.\nI haven’t mentioned a scale yet. Many people include a scale in the photos and then you can scale the model directly to the included scale. I don’t like it because I find it makes masking harder. My solution is to measure a feature of the object and then scale the object later. This post is a bit outdated now (I use Blender which has changed a lot), but it describes my process.\nOne challenge is how to place the object. You can use a stand, putty, or set the object directly on the turntable, or on something else to hold it slightly off the turntable (having the object off the turntable reduces shadows). This process is very dependent on the object you want to capture.\n\n\nTaking pictures\n\nSettings\nUse manual settings on the camera. The fewer settings (ideally none) that change the better. You don’t even want the focus to change. My wife is a photographer but it wasn’t until I learned photogrammetry that I picked up how to use a camera manually. There are four key things to control–and they’re all related (changing one may require changing another):\n\nFocus\nAperture\nISO\nShutter speed\n\nYou want as much of the object in focus as possible. Manually focus on the center of the object, and then don’t change the focus (turn autofocus off). The aperture (f-stop/f-number) controls the depth of field. This describes how much of the object is in focus at the same time. A low f-stop has a shallow depth of field, so you want a high f-stop. Be careful with the f-stop though. One mistake I used to make was using too high of an f-stop. If the f-stop is too high then the physics of light diffraction will make the images blurrier (Google it if you want to know why). You want the lowest f-stop that keeps the entire–or at least most–of the object in focus.\nISO is a balance between brightness and graininess. A low ISO gives you less grainy images but requires more light. A high ISO allows you to use less light but you will have grainy pictures. Set up your lighting to allow the lowest ISO.\nShutter speed is how long the camera shutter stays open. Longer exposures lead to brighter images. If you have a high f-stop then you let in less light, and if you have a low ISO you have a darker photo. The balance to these is to have a longer shutter-speed. If something moves while the shutter is open, then you will have a blurry photo. With a tripod and a stationary object on a turntable, you shouldn’t have to worry about how long the shutter stays open. The drawback is that longer shutter speeds means it takes longer to take the picture. More lighting can help with this though.\nYou’ll need to experiment with these settings until you have a photo that is sharp and not too dark. Once you have the settings right, don’t change anything until you’re done with the shoot.\n\n\nOrientation\nTake the photos from a high and low angles: around 45 degrees for high and then from an angle shallow enough to capture the side and some of the top. The can angles vary depending on the object. I like to take about 20 pictures as I move the object in a full circle. That translates to rotating the turntable 18 degrees per photo, but often I just eye ball it. If I’m shooting directly at a sharp edge then I will take more photos as I begin to directly face teh sharp edge (shooting directly at a straight edge isn’t very helpful). Usually I’ll set up the camera for a high angle, take the pictures, flip the object over, and then move the camera to a low-angle position and repeat that step. But it might be easier to move the camera than flip the camera over.\nWith this process I end up with about 80 photos. This is usually more than necessary, but not always. If any part of the object isn’t well captured in your photos then take as many additional photos as you need. You don’t have to use every photo in the 3D model, but if you need more photos then you have to do a lot more work.\n\n\n\nFormat and resolution\nThe more data the better, right? Larger files use more storage space and higher resolutions require longer processing times. You can always downsize an image but you can’t increase resolution without interpolating…and don’t bother trying that. I always choose the highest resolution. You can save your files in raw format for best results, but tif or png will work as well. Yes jpg also works, but I recommend against using a lossy format (meaning that jpgs lose some of the information you save–and yes, the tutorial files are in jpg because the data is stored on my website and needs to be as small as possible but I’ve still got the full-resolution tifs). You don’t even need to downsize images to speed up processing because Metashape can do that for you if you want."
  },
  {
    "objectID": "posts/Artifact-Photogrammetry-Basics/Artifact-Photogrammetry-Basics.html#masks",
    "href": "posts/Artifact-Photogrammetry-Basics/Artifact-Photogrammetry-Basics.html#masks",
    "title": "Artifact Photogrammetry Basics",
    "section": "Masks",
    "text": "Masks\nI like to use photoshop to make my masks. You can even automate the process. What I do first is copy the photos into a new folder labeled masks. I then edit each photo in the masks folder and save it as is. It may help to add an _mask tag to each filename, but it’s up to you. The mask process is simple. Essentially you just need to select the object using your favorite selection tool. Use the fill tool to make the object white. Invert your selection. Then use the fill tool to make everything else black. Check my post on automating the process to see more details.\n\n\n\nMask"
  },
  {
    "objectID": "posts/Artifact-Photogrammetry-Basics/Artifact-Photogrammetry-Basics.html#import-images-and-masks",
    "href": "posts/Artifact-Photogrammetry-Basics/Artifact-Photogrammetry-Basics.html#import-images-and-masks",
    "title": "Artifact Photogrammetry Basics",
    "section": "Import images and masks",
    "text": "Import images and masks\nNow we can open metashape. If you don’t have it, then download it on a free trial basis to start. You can download some reduced-resolution photos to follow along or include your own. Click on the Workflow drop down menu and select Add Photos. Select all of your photos and import them. Next, we need to add the masks. You’ll see a folder labeled Cameras under Chunk 1 in the workspace on the left. Click the arrow next to the Cameras folder to see all of the images. Right click on one (pick a photo, any photo) and find the option labeled Masks. Click Import Masks. You will see a menu like the one below. Your method should be From File. The operation should be Replacement. If you are using the test dataset then the filename should look like {filename}.jpg. What the filename structure means is that the software will look for a file with the same name as the images we already imported. If your image is A1.jpg and your mask is called A1_mask.jpg then the Filename template should look like {filename}_mask.jpg. Tolerance doesn’t apply to this operation and you want to make sure the All cameras option is selected in the Apply to section. Press ok, and you can then select the folder where the masks are stored. You should now see the non-object areas of your images grayed out.\n\n\n\nImport Masks"
  },
  {
    "objectID": "posts/Artifact-Photogrammetry-Basics/Artifact-Photogrammetry-Basics.html#align-cameras",
    "href": "posts/Artifact-Photogrammetry-Basics/Artifact-Photogrammetry-Basics.html#align-cameras",
    "title": "Artifact Photogrammetry Basics",
    "section": "Align cameras",
    "text": "Align cameras\n\n\n\nTie Points\n\n\nGo back to the Workflow menu and choose Align Photos. You’ll see a menu like the one below. You haven’t had to make any real decisions in the workflow up to this point, but now we have a lot of options. Aligning the photos is the most important step in the 3D modeling process. If this step works well then you will get a good 3D model, otherwise you’ll need new photos. Don’t get discouraged if your photos don’t align on the first try though! It can take a lot of playing with the settings if you have a difficult object, but you can get good results with bad photographs (see this article for archaeological examples). While you want your photos to align nicely the first try, higher settings shouldn’t be your first option and you can sometimes get better results with lower settings. The Accuracy option controls whether the image is reduced in size (your original images are not modified) before matching begins. I would try medium first and then work to higher settings if you get bad results. Higher settings can significantly increase the time it takes to align photos. Each of the following steps can take seconds to hours depending on the number of images, masks, and settings. Each of the steps below took less than a minute on my machine, but it may take much longer. You can check the manual for descriptions of every option, as I’ll only point out the ones that you need to change or double check. For this step, just choose your accuracy and make sure that the Apply masks to option reads Key points in the Advanced section. Hit ok and let the software run the calculations.\nClick the show cameras option on the menu bar (looks like a camera) if you don’t see the positions of your camera next to the tie points that were just generated. The tie points should generally resemble your object, but don’t worry if some points are scattered about in the wrong positions. You can also check the positions of the cameras around the object. If one looks out of place then you can remove it, disable it, or (best option) try to realign it by right clicking on the photo and using the options listed there. If not all cameras are aligned, the first step I recommend is to try aligning again with higher settings.\n\n\n\nAlign Photos"
  },
  {
    "objectID": "posts/Artifact-Photogrammetry-Basics/Artifact-Photogrammetry-Basics.html#build-dense-cloud",
    "href": "posts/Artifact-Photogrammetry-Basics/Artifact-Photogrammetry-Basics.html#build-dense-cloud",
    "title": "Artifact Photogrammetry Basics",
    "section": "Build dense cloud",
    "text": "Build dense cloud\n\n\n\nDense Cloud\n\n\nIf all your cameras seem aligned then build your dense cloud by going back to the Workflow menu and choosing Build Dense Cloud. I would start with high quality in this case, as the photos are already reduced. I also usually choose Aggressive for the Depth filtering shown in the Advanced section. Press ok and you should get a result (this part will take the longest) that looks a lot like what you are trying to model.\n\n\n\nBuild Dense Cloud"
  },
  {
    "objectID": "posts/Artifact-Photogrammetry-Basics/Artifact-Photogrammetry-Basics.html#build-mesh",
    "href": "posts/Artifact-Photogrammetry-Basics/Artifact-Photogrammetry-Basics.html#build-mesh",
    "title": "Artifact Photogrammetry Basics",
    "section": "Build mesh",
    "text": "Build mesh\n\n\n\n3D Model No Texture\n\n\nAs you might guess, the next step is to go back to the Workflow menu and choose the next option–Build Mesh. I always choose a high Face count at this step. I leave everything else at its default. For some objects you will need to disable Interpolation in the Advanced section. This will attempt to fill holes which is often necessary, but sometimes it can distort the model and create an innacurate representation. If the model is meant for precise geometric morphometrics or other analysis then interpolation may cause problems. You should now have a 3D model that should even be 3D printable.\n\n\n\nBuild Mesh"
  },
  {
    "objectID": "posts/Artifact-Photogrammetry-Basics/Artifact-Photogrammetry-Basics.html#add-texture",
    "href": "posts/Artifact-Photogrammetry-Basics/Artifact-Photogrammetry-Basics.html#add-texture",
    "title": "Artifact Photogrammetry Basics",
    "section": "Add texture",
    "text": "Add texture\n\n\n\n3D Model Texture\n\n\nThe last step in the photogrammetry process is to add a texture. This does nothing to the model from a 3D standpoint, but it does add a nice, photorealistic touch that significantly improves the look of your model. Select the Build Texture option from the Workflow menu, and leave everything at the default. In some cases, you may want to increase the texture size for more detail. You should notice much clearer looking details on the model now.\n\n\n\nTexture\n\n\nThis is what your model should look like if you used the example files (except this model was made on full-resolution photos):"
  },
  {
    "objectID": "posts/Artifact-Photogrammetry-Basics/Artifact-Photogrammetry-Basics.html#export",
    "href": "posts/Artifact-Photogrammetry-Basics/Artifact-Photogrammetry-Basics.html#export",
    "title": "Artifact Photogrammetry Basics",
    "section": "Export",
    "text": "Export\nThere are other options to manipulate the model in Metashape that can be useful. 3D models are often huge files and your computer may struggle to render it. You can decimate the mesh (reduce the number of triangles) by going to Tools &gt; Mesh &gt; Decimate Mesh. Choose a target face count and the model size will be reduced. The great thing about photogrammetry is that as long as you have the original photos you can reproduce your workflow and create higher resolution models.\nOnce you are satisified with the model you can export it to a number of formats. Choose File &gt; Export &gt; Export Model. My default format is OBJ as it allows you to export the model texture. I usually use the defaults, except I like to choose png for the texture resolution. I will then use Blender to import the model and resize it or clean up bad parts of the model. Meshlab and Cloud Compare are other, open-sourced software that have a lot of great tools for working with 3D models. The easiest to view and share option is to save as an Adobe pdf. If you want to 3D print it, then export to STL.\n\n\n\nExport"
  },
  {
    "objectID": "posts/Artifact-Photogrammetry-Basics/Artifact-Photogrammetry-Basics.html#next-steps",
    "href": "posts/Artifact-Photogrammetry-Basics/Artifact-Photogrammetry-Basics.html#next-steps",
    "title": "Artifact Photogrammetry Basics",
    "section": "Next steps",
    "text": "Next steps\nNow that you have a 3D model what do you do? Share it, print it, and/or analyze it. Sketchfab is probably the easiest way to share your model, although they’ve made it harder to do anything for free lately. 3D printers are a lot cheaper than they used to be, but some libraries now have 3D printing services and there are many online options for 3D printing things. The field of geometric morphometrics has a lot of exciting developments for analyzing shapes and is the next step to explore if you want to start analyzing your 3D models.\nIf you have any questions, comments, or tips to share, please let me know in the comments or via email."
  },
  {
    "objectID": "posts/Automatic-Background-Removal-for-Photogrammetry/Automatic-Background-Removal-for-Photogrammetry.html",
    "href": "posts/Automatic-Background-Removal-for-Photogrammetry/Automatic-Background-Removal-for-Photogrammetry.html",
    "title": "Automatic Background Removal",
    "section": "",
    "text": "There are a few different ways to automatically mask backgrounds when making a 3D model using photogrammetry, but I’ve struggled to find a method that worked for what I was doing. Photoshop released a new AI feature to automatically select the subject of an image. This post explores how this tool can be used to mask unwanted background features when creating 3D models using photogrammetry. This is not a photogrammetry tutorial, and I assume a basic familarity with Metashape (formerly Photoscan), the software I use for photogrammetry.\nMy standard process when creating a 360° 3D model of an object is to create a black and white mask of each image by using Photoshop’s Quick Selection Tool, or other selection tools when necessary, to select the object, make it white, and then make everything else black. I use Metashape’s create mask from file option to automatically apply each mask to its corresponding image. This is usually the part of my workflow that requires the most work from me. Running the model can take longer, but I don’t need to do much. Photoshop action scripts speed up the process of making a mask, as I can record the actions to make the image black and white once I’ve selected the object.\nI’ve used a similar process to automatically create masks using the new select subject option. Spoiler, the results aren’t as good as doing it by hand, but we’re not quite living in the AI world of my dreams yet.\nYou’ll need at least Photoshop version 19.1 (I’m using version 20.0.4). It helps to be familiar with Photoshop, and I recommend looking up some basic tutorials if you are not familiar with it.\nThe model I’ll be making is of a small ceramic seed jar found on the surface of an archaeology site I was visiting in western New Mexico. I just had my smartphone on me (a Pixel XL 2), but I snapped several photos with the jar sitting on the ground, and then I held the jar in my hand and rotated it. This should be a challenging test. I didn’t have much time and the lighting was not great. Having my hand in many of the photos should be the most challenging aspect to remove automatically."
  },
  {
    "objectID": "posts/Automatic-Background-Removal-for-Photogrammetry/Automatic-Background-Removal-for-Photogrammetry.html#create-photoshop-action",
    "href": "posts/Automatic-Background-Removal-for-Photogrammetry/Automatic-Background-Removal-for-Photogrammetry.html#create-photoshop-action",
    "title": "Automatic Background Removal",
    "section": "Create Photoshop Action",
    "text": "Create Photoshop Action\nThe first thing to do is to make a copy of all of the photos you’ll be using and place them in a new folder labeled masks. Open one of these images in Photoshop. Access the Actions menu by pressing F9, and press the Create new action button.\n\n\n\nNew action\n\n\nI named my action Automatic Mask. Press the Record button and now Photoshop will keep track of everything we do. Press w to access the Quick Selection Tool and you should see the Select Subject button at the top of the screen.\n\n\n\nSelect subject\n\n\nPress that button and you should see the most prominent object selected. The selection worked fairly well. It’s not surprising the out of focus ground was removed, but I wasn’t sure that my hand would be left out of the selection.\n\n\n\nSelect subject results\n\n\nNow go to Edit&gt;Fill and make sure the Content is white, the Blending Mode is Normal, and the Opacity is 100%. Press Ok and the object should be white. Now invert the selection by pressing Ctrl+Shift+I, and repeat the Edit&gt;Fill process except this time choose black. You should now have a black and white mask.\n\n\n\nBlack and white mask\n\n\nJust save the image and then close it. Make sure those two actions are separate as we are recording them and for some reason closing and saving at the same time doesn’t work well. We are still recording, so open the image again, access the Actions menu and press the Stop recording button. You’ll then have to delete the Open step. If you cannot see the steps in the action then click the dropdown arrow next to the action name. Select Open and press the Delete button. You should see six steps once you’re done.\n\nSelect Subject\nFill\nInverse\nFill\nSave\nClose\n\n\n\n\nDelete open step\n\n\nNow close the image again."
  },
  {
    "objectID": "posts/Automatic-Background-Removal-for-Photogrammetry/Automatic-Background-Removal-for-Photogrammetry.html#run-the-action",
    "href": "posts/Automatic-Background-Removal-for-Photogrammetry/Automatic-Background-Removal-for-Photogrammetry.html#run-the-action",
    "title": "Automatic Background Removal",
    "section": "Run the Action",
    "text": "Run the Action\nPhotoshop allows you to run the action on each photo in a folder automatically. Go to File&gt;Automate&gt;Batch. The last action you made should automatically be selected, but select your action if it is not already. Choose the folder you saved the images in (make sure it is the folder for masks, not the original images (make sure you have backups just in case). The Destination should be Save and Close, and the Override Action “Save As” Commands box should be checked. Press Ok and it will repeat this action for every image. \nNow each image should be a black and white mask. You can see that not every one of these images was perfectly masked, but you can see from the model I made that the masks worked well enough, and this process saved me a lot of time.\n\n\n\nMask results"
  },
  {
    "objectID": "posts/Automatic-Background-Removal-for-Photogrammetry/Automatic-Background-Removal-for-Photogrammetry.html#the-model",
    "href": "posts/Automatic-Background-Removal-for-Photogrammetry/Automatic-Background-Removal-for-Photogrammetry.html#the-model",
    "title": "Automatic Background Removal",
    "section": "The Model",
    "text": "The Model\nIt’s not a perfect model, but I’ve never tried to make a model from an object held in my hand before, so I’ll call this a good result for the circumstances. Feel free to comment if you found this useful or if you have some tips on improvements or a better process.\n\n\n\n\nSmall Seed Jar by Robert Bischoff on Sketchfab"
  },
  {
    "objectID": "posts/Easy-GitHub-Pages-Website/Easy-GitHub-Pages-Website.html",
    "href": "posts/Easy-GitHub-Pages-Website/Easy-GitHub-Pages-Website.html",
    "title": "Easy GitHub Pages Website",
    "section": "",
    "text": "The Association of All Graduate Students at the School of Human Evolution and Social Change at Arizona State University held a workshop led by Maryse Biernat on the topic of digital portfolios–essentially building your own website. My contribution was demonstrating how I built this website.\nI chose to use GitHub because I didn’t like the restrictions of WordPress. I spent some frustrating hours trying to build something from scratch and then went the smart route and forked Barry Clark’s respository barryclark/jekyll-now. I followed the tutorial he gives there to get my own website up and running. But, I wanted a more academic-focused site for the workshop, so I created this example repository rjbischo/rjbischo.github.io.\nThe steps are essentially the same as what are in Clark’s repository, but here is the gist of it, and you can be up and running in minutes by changing a few things in the files. The main prerequisite is that you need to know a little about git, but you can get started with the GitHub Desktop app and that’s all you’ll need.\nSteps to create a GitHub pages website:\n1. Create an account (your username will be your website address–{username}.github.io)\n2. Fork this repository – https://github.com/rjbischo/rjbischo.github.io\n3. Rename the repository to {your username}.github.io\n4. Clone the repository\n5. Update the _config.yml, about.html, CV.html, CV.pdf, publications.html, _includes/default.html and images to whatever you want them to be.\n6. Push the repository to GitHub.\n7. Go to your new website at {your username}.github.io\nFor your reference, here are some example GitHub pages:\nhttps://github.com/collections/github-pages-examples"
  },
  {
    "objectID": "posts/Easy-GitHub-Pages-Website/Easy-GitHub-Pages-Website.html#conclusion",
    "href": "posts/Easy-GitHub-Pages-Website/Easy-GitHub-Pages-Website.html#conclusion",
    "title": "Easy GitHub Pages Website",
    "section": "Conclusion",
    "text": "Conclusion\nI like that GitHub pages is free, you can be up and running in minutes, and it is endlessly customizable. The drawback is customizing it is not as user friendly as some other sites, but that can be part of the fun as you learn new things."
  },
  {
    "objectID": "posts/Making-Image-Mosaics-in-R/Making-Image-Mosaics-in-R.html",
    "href": "posts/Making-Image-Mosaics-in-R/Making-Image-Mosaics-in-R.html",
    "title": "Making Image Mosaics in R",
    "section": "",
    "text": "I had the idea to make my dad an image mosaic of his grandkids for Father’s day. My immediate thought was to check if R had a package for that–and it does. The RsimMosaic package is a great tool for making image mosaics. Combined with the magick package, I have all the tools I need. While I think the picture of my dad turned out great, I decided to make an archaeology related mosaic for this post.\n\n\n\nMosaic Small\n\n\nThe first step was to gather the photos. The more photos the better. My wife and I use Google Photos to store all of our pictures, so I searched the term archaeology within my photo account and used Google’s AI to make my life easier. I downloaded more than 1,000 photographs that matched this term. Not every picture was archaeology related, but it did a fairly good job. I only removed a few pictures that didn’t fit.\nI also use tidyverse which is standard for me and magrittr, as I like some of the pipes not included in tidyverse. The tictoc package is a great little package for keeping track of elapsed time. Base R can take care of a lot of the functions in these packages, but I find them more convenient and easier to follow. The magick package sometimes has a problem with memory management when communicating with imagemagick, but the development version of the package has an image_destroy function that helps.\n# libraries used\nremotes::install_github('ropensci/magick')\nlibrary(RsimMosaic)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(magick)\nlibrary(tictoc)\nAs my photos were zipped, I used R to unzip them (why not use R for everything, right?). I use the walk function from purrr as it doesn’t return any results. I have the photos stored in a subdirectory called Photos.\n# photos are zipped; list files and unzip them\nlfZip &lt;- list.files(\"Photos\", full.names = T, pattern = 'zip')\n\n# use walk from purrr to unzip folders\nwalk(lfZip, unzip, exdir = 'Photos')\n\n# list all photos\nlf &lt;- list.files(\"Photos\", pattern = 'jpg', full.names = T)\nThe biggest problem with the image mosaic is that all of the images have to be square. I don’t know about you, but I don’t take many square photos. RsimMosaic does have a createTiles function, but as the help file notes, the interpolation scheme does not produce high quality images. I wrote my own function to generate square tiles and again I used walk to save the results. The function crops photos in the middle, depending on whether it is a landscape or portrait orientation.\n# create new directory to save results\ndir.create('Tiles')\n\n# function to create square tiles\nsquareTiles &lt;- function(.x, size = 60){\n  img &lt;- image_read(.x)\n  info &lt;- img %&gt;% image_info\n  if(info$width &gt; info$height){\n    img %&lt;&gt;% image_scale(str_glue('x{size}'))\n    img %&gt;%\n      image_crop(str_glue(\n        '{size}x{size}+{(image_info(img)$width - {size}) / 2}+0')) %&gt;%\n      image_write(str_replace_all(.x,'Photos','Tiles'))\n  } else {\n    img %&lt;&gt;% image_scale(str_glue('{size}'))\n    img %&gt;%\n      image_crop(str_glue(\n        '{size}x{size}+0+{(image_info(img)$height - {size}) / 2}')) %&gt;%\n      image_write(str_replace_all(.x,'Photos','Tiles'))\n  }\n  image_destroy(img) # prevent memory problems\n}\n\n# run function\ntic()\nwalk(lf, squareTiles)\ntoc()\n# 4837.13 sec elapsed\nIt took my computer 4837 seconds (r round(4837 / 60,2) hours) to run for all 1,486 pictures. Maybe I downloaded too many, but this is going to be a lengthy process.\nHere’s a comparison of two 60-pixel images, the first is from the createTiles function, and the second is from the custom function. Magick is the clear winner.\n\n\n\n\n\n\n\ncreateTiles\nmagick\n\n\n\n\n\n\n\n\n\nOnce the tiles were ready, finally. I picked one of the photos, a picture of Pueblo Bonito, and converted it to a smaller size. Each pixel in the input image will be replaced by a 60-pixel tile, which means the resulting image is huge unless you use a very small pixel value. I’m going for big, but not too big.\n\n\n\nPueblo Bonito\n\n\nimage_read('Photos/20171006_135349.jpg') %&gt;% image_scale('250') %&gt;%\n  image_write('PuebloBonito.jpg')\n\n# Create the mosaic\ntic()\ncomposeMosaicFromImageRandom('PuebloBonito.jpg', 'PuebloBonitoMosaic.jpg',\n                             # 'Tiles', removeTiles=FALSE)\ntoc()\n# 93.66 sec elapsed\nNote in case you run across this: When I ran this function the first time it failed. I did some investigation and determined that a handful of photos were saved with an 8 bit depth and the other were 24 bit depth. As there were only a few, I just removed them and proceeded.\nThe result is cool if you zoom in, but doesn’t look all that great. A good way to improve the look is to overlay the original photo with a 50% opacity. I also wanted to increase the contrast. I do love Photoshop and it can easily be done that route, but I think I said something about doing it all in R, so here we go.\n(Warning: the following images may take a long time to load)\n\n\n\nPueblo Bonito Mosaic\n\n\nIncreasing contrast is not difficult, but the opacity problem is more challenging. There is no native way in the magick package to change the background opacity, and because it is a jpg image, the color scheme is rgb when it needs to be rgba. The hack I used is to make all white pixels transparent on the original photo, which doesn’t really change the picture but does change it to rgba. I then modified the bitmap directly by multiplying it by 0.5. Then the images can be combined and saved to produce the final result.\noriginal &lt;- image_read('Photos/20171006_135349.jpg')\nmosaic &lt;- image_read('PuebloBonitoMosaic.jpg') %&gt;%\n  image_contrast() # increase contrast\ninf &lt;- mosaic %&gt;% image_info\noriginal %&lt;&gt;% image_scale(str_glue('{inf$width}x{inf$height}!')) %&gt;%\n  image_transparent('white')\nbitmap &lt;- original[[1]]\nbitmap[4,,] &lt;- as.raw(as.integer(bitmap[4,,]) * 0.5)\noriginal &lt;- image_read(bitmap)\noriginal &lt;- c(mosaic, original) %&gt;% image_flatten()\nimage_write(original, 'PuebloBonitoMosaicMod.jpg', quality = 80)\n\n\n\nPueblo Bonito Overlay\n\n\nIt’s not perfect, but there are a lot of ways to tweak it to improve the result. Changing the opacity, and pixel sizes will make the biggest difference. Let me know if you have any ideas on how to improve this code, and I hope you have some fun making mosaics of your dogs, cats, or whatever you love."
  },
  {
    "objectID": "posts/Pilling Figurines 3D Models/Pilling-Figurines-3D-Models.html",
    "href": "posts/Pilling Figurines 3D Models/Pilling-Figurines-3D-Models.html",
    "title": "Pilling Figurines 3D Models",
    "section": "",
    "text": "I was recently able to photograph the Pilling Figurines and process the images into 3D models using Agisoft Photoscan.\nThe 12 Pilling Figurines are on display in the Utah State University Eastern Prehistoric Museum in Price, Utah. Thanks goes to Tim Riley and the museum for allowing access. These figurines and the rest of the museum is well worth the visit if you’re going through Price."
  },
  {
    "objectID": "posts/Pilling Figurines 3D Models/Pilling-Figurines-3D-Models.html#links-to-articles-discussing-the-pilling-figurines",
    "href": "posts/Pilling Figurines 3D Models/Pilling-Figurines-3D-Models.html#links-to-articles-discussing-the-pilling-figurines",
    "title": "Pilling Figurines 3D Models",
    "section": "Links to articles discussing the Pilling Figurines:",
    "text": "Links to articles discussing the Pilling Figurines:\nhttps://en.wikipedia.org/wiki/Pilling_Figurines\nhttp://etv10news.com/bud-pilling-shares-story-of-rare-discovery-of-fremont-artifacts/\nhttp://archive.sltrib.com/article.php?id=53879624&itype=CMSID"
  },
  {
    "objectID": "posts/Pilling Figurines 3D Models/Pilling-Figurines-3D-Models.html#d-models",
    "href": "posts/Pilling Figurines 3D Models/Pilling-Figurines-3D-Models.html#d-models",
    "title": "Pilling Figurines 3D Models",
    "section": "3D Models",
    "text": "3D Models\n\n&lt;iframe width=\"640\" height=\"480\" src=\"https://sketchfab.com/models/45e036d22eb0430c840751b5ae8deeb7/embed\" frameborder=\"0\" allow=\"autoplay; fullscreen; vr\" mozallowfullscreen=\"true\"\n        webkitallowfullscreen=\"true\"&gt;\n&lt;/iframe&gt;\n&lt;p style=\"font-size: 13px; font-weight: normal; margin: 5px; color: #4A4A4A;\"&gt;\n    &lt;a href=\"https://sketchfab.com/models/45e036d22eb0430c840751b5ae8deeb7?utm_medium=embed&utm_source=website&utm_campain=share-popup\"\n       target=\"_blank\"\n       style=\"font-weight: bold; color: #1CAAD9;\"&gt;\n        Pilling Figurine No. 1\n    &lt;/a&gt;\n    by\n    &lt;a href=\"https://sketchfab.com/rbischoff?utm_medium=embed&utm_source=website&utm_campain=share-popup\"\n       target=\"_blank\"\n       style=\"font-weight: bold; color: #1CAAD9;\"&gt;\n        Robert Bischoff\n    &lt;/a&gt;\n    on\n    &lt;a href=\"https://sketchfab.com?utm_medium=embed&utm_source=website&utm_campain=share-popup\"\n       target=\"_blank\"\n       style=\"font-weight: bold; color: #1CAAD9;\"&gt;\n        Sketchfab\n    &lt;/a&gt;\n&lt;/p&gt;\n\n\n\n\n\n\nPilling Figurine No. 2 by Robert Bischoff on Sketchfab\n\n\n\n\n\n\n\nPilling Figurine No. 3 by Robert Bischoff on Sketchfab\n\n\n\n\n\n\n\nPilling Figurine No 4 by Robert Bischoff on Sketchfab\n\n\n\n\n\n\n\nPilling Figurine No. 5 by Robert Bischoff on Sketchfab\n\n\n\n\n\n\n\nPilling Figurine No. 6 by Robert Bischoff on Sketchfab\n\n\n\n\n\n\n\nPilling Figurine No. 7 by Robert Bischoff on Sketchfab\n\n\n\n\n\n\n\nPilling Figurine No. 8 by Robert Bischoff on Sketchfab\n\n\n\n\n\n\n\nPilling Figurine No. 9 by Robert Bischoff on Sketchfab\n\n\n\n\n\n\n\nPilling Figurine No 10 by Robert Bischoff on Sketchfab\n\n\n\n\n\n\n\nPilling Figurine No. 11 by Robert Bischoff on Sketchfab\n\n\n\n\n\n\n\nPilling Figurine No. 12 by Robert Bischoff on Sketchfab"
  },
  {
    "objectID": "posts/UseR-Conference-Shiny-Presentation/UseR-Conference-Shiny-Presentation.html",
    "href": "posts/UseR-Conference-Shiny-Presentation/UseR-Conference-Shiny-Presentation.html",
    "title": "UseR Conference Shiny Presentation",
    "section": "",
    "text": "Today I had the opportunity to present the work I’ve done with Dan Hruschka on the CatMapper application in the virtual UseR conference. The app was built using R shiny and Neo4j.\n\n\n\nCatMapper walk-through\n\n\nYou can download the powerpoint here"
  }
]